{
    "output_dir": "runs/llama3.2_1B_peft_1/",
    "num_train_epochs": 1,
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 4,
    "eval_steps": 1000,
    "save_steps": 1000,
    "save_total_limit": 3,
    "eval_on_start": false,
    "eval_strategy": "steps",
    "eval_accumulation_steps": 8,
    "fp16": true,
    "optim": "adafactor",
    "torch_compile": false,
    "report_to": ["tensorboard"]
}
